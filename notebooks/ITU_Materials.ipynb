{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a6de4075",
   "metadata": {},
   "source": [
    "# Experiments with Measured Data - ITU Materials\n",
    "\n",
    "This notebooks fits the power scaling for the \"ITU Materials\" baseline of Section IV-C of the paper [\"Learning Radio Environments by\n",
    "Differentiable Ray Tracing\"](https://arxiv.org/abs/2311.18558) by J. Hoydis, F. Ait Aoudia, S. Cammerer, F. Euchner, M. Nimier-David, S. ten Brink, and A. Keller, Dec. 2023.\n",
    "\n",
    "The code is made available under the [NVIDIA License](https://github.com/NVlabs/diff-rt-calibration/blob/main/LICENSE.txt).\n",
    "\n",
    "To run this notebook, you need first to:\n",
    "\n",
    "- Download the \"dichasus-dc01.tfrecords\" file from the [DICHASUS website](https://dichasus.inue.uni-stuttgart.de/datasets/data/dichasus-dcxx/) to the folder `data/tfrecords` within the cloned repository. More information about the DICHASUS channel sounder can be found [here](https://arxiv.org/abs/2206.15302).\n",
    "\n",
    "- Create a dataset of traced paths using the script [gen_dataset.py](../code/gen_dataset.py). For this purpose, ensure that you are in the `code/` folder, and run:\n",
    "```bash\n",
    "python gen_dataset.py -traced_paths_dataset dichasus-dc01 -traced_paths_dataset_size 10000\n",
    "```\n",
    "This script stores the generated dataset in the `data/traced_paths/` folder.\n",
    "Generating the dataset of traced paths can take a while."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fe098cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "gpu_num = 0 # Use \"\" to use the CPU\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = f\"{gpu_num}\"\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "import tensorflow as tf\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        tf.config.experimental.set_memory_growth(gpus[0], True)\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "\n",
    "%matplotlib widget\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "sys.path.append('../code')\n",
    "\n",
    "import sionna\n",
    "from utils import *\n",
    "import datetime # For logging\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97e27d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_name = \"itu_materials\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c89ed399",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset\n",
    "dataset_name = '../data/traced_paths/dichasus-dc01'\n",
    "dataset_filename = os.path.join(dataset_name + '.tfrecords')\n",
    "params_filename = os.path.join(dataset_name + '.json')\n",
    "\n",
    "# Configure training parameters and step\n",
    "batch_size = 8\n",
    "num_iterations = 10000\n",
    "delta = 0.999 # Parameter for exponential moving average\n",
    "\n",
    "# Size of validation set size\n",
    "# The validation set is used for early stopping, to ensure\n",
    "# training does not overfit.\n",
    "validation_set_size = 100\n",
    "# We don't use the test set here, but need is size for splitting\n",
    "test_set_size = 4900\n",
    "\n",
    "# Sizes of the training set to evaluate\n",
    "training_set_size = 5000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8285052",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(params_filename, 'r') as openfile:\n",
    "    params = json.load(openfile)\n",
    "\n",
    "# Scene\n",
    "scene_name = params['scene_name']\n",
    "# Size of the dataset\n",
    "dataset_size = params['traced_paths_dataset_size']\n",
    "\n",
    "num_subcarriers = 1024\n",
    "bandwidth = 50e6\n",
    "frequencies = subcarrier_frequencies(num_subcarriers, bandwidth/num_subcarriers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5fd3faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the TF records as a dataset\n",
    "dataset = tf.data.TFRecordDataset([dataset_filename]).map(deserialize_paths_as_tensor_dicts)\n",
    "\n",
    "# Split the dataset\n",
    "# We don't use the test set\n",
    "training_set, validation_set, _ = split_dataset(dataset, dataset_size, training_set_size, validation_set_size, test_set_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5656b95b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "\n",
    "    # Training set\n",
    "    training_set_iter = iter(training_set.shuffle(256, seed=42).batch(batch_size).repeat(-1))\n",
    "\n",
    "    # Validation set\n",
    "    validation_set_iter = iter(validation_set.batch(batch_size).repeat(-1))\n",
    "    num_validation_iter = validation_set_size // batch_size\n",
    "\n",
    "    # Load the scene\n",
    "    scene = init_scene(scene_name, use_tx_array=True)\n",
    "\n",
    "    # Place the transmitters\n",
    "    place_transmitter_arrays(scene, [1,2])\n",
    "\n",
    "    # Instantitate receivers\n",
    "    instantiate_receivers(scene, batch_size)\n",
    "\n",
    "    scaling_factor = tf.Variable(6e-9, dtype=tf.float32, trainable=False)\n",
    "\n",
    "    # Setting up tensorboard\n",
    "    current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    train_log_dir = os.path.join('../tb_logs/', training_name, current_time)\n",
    "    train_summary_writer = tf.summary.create_file_writer(train_log_dir)\n",
    "    # Checkpoint\n",
    "    weights_filename = os.path.join('../checkpoints/', training_name)\n",
    "\n",
    "    @tf.function\n",
    "    def training_step(rx_pos, h_meas, traced_paths):\n",
    "\n",
    "        # Placer receiver\n",
    "        set_receiver_positions(scene, rx_pos)\n",
    "\n",
    "        # Build traced paths\n",
    "        traced_paths = tensor_dicts_to_traced_paths(scene, traced_paths)\n",
    "\n",
    "\n",
    "        # Compute paths fields\n",
    "        paths = scene.compute_fields(*traced_paths,\n",
    "                                     scat_random_phases=False,\n",
    "                                     check_scene=False)\n",
    "\n",
    "        a, tau = paths.cir(scattering=False) # Disable scattering\n",
    "\n",
    "        # Compute channel frequency response\n",
    "        h_rt = cir_to_ofdm_channel(frequencies, a, tau)\n",
    "\n",
    "        # Remove useless dimensions\n",
    "        h_rt = tf.squeeze(h_rt, axis=[0,2,5])\n",
    "\n",
    "        # Normalize h to make sure that power is independent of the number of subacrriers\n",
    "        h_rt /= tf.complex(tf.sqrt(tf.cast(num_subcarriers, tf.float32)), 0.)\n",
    "\n",
    "        # Compute scaling factor\n",
    "        scaling_factor.assign(delta*scaling_factor + (1-delta)*mse_power_scaling_factor(h_rt, h_meas))\n",
    "\n",
    "        # Scale measurements\n",
    "        h_meas *= tf.complex(tf.sqrt(scaling_factor), 0.)\n",
    "\n",
    "        # Compute losses\n",
    "        h_rt = sionna.utils.flatten_dims(h_rt, 3, 0)\n",
    "        h_meas = sionna.utils.flatten_dims(h_meas, 3, 0)\n",
    "        # Compute the average of a power and delay spread loss\n",
    "        loss_ds = delay_spread_loss(h_rt, h_meas)\n",
    "        loss_pow = power_loss(h_rt, h_meas)\n",
    "        loss_ds_pow = loss_ds + loss_pow\n",
    "\n",
    "        return loss_ds_pow, loss_ds, loss_pow, scaling_factor\n",
    "\n",
    "    @tf.function\n",
    "    def evaluation_step(rx_pos, h_meas, traced_paths, scaling_factor):\n",
    "\n",
    "        # Placer receiver\n",
    "        set_receiver_positions(scene, rx_pos)\n",
    "\n",
    "        # Build traced paths\n",
    "        traced_paths = tensor_dicts_to_traced_paths(scene, traced_paths)\n",
    "\n",
    "        paths = scene.compute_fields(*traced_paths,\n",
    "                                     scat_random_phases=False,\n",
    "                                     check_scene=False)\n",
    "\n",
    "        a, tau = paths.cir(scattering=False) # Disable scattering\n",
    "\n",
    "        # Compute channel frequency response\n",
    "        h_rt = cir_to_ofdm_channel(frequencies, a, tau)\n",
    "\n",
    "        # Remove useless dimensions\n",
    "        h_rt = tf.squeeze(h_rt, axis=[0,2,5])\n",
    "\n",
    "        # Normalize h to make sure that power is independent of the number of subacrriers\n",
    "        h_rt /= tf.complex(tf.sqrt(tf.cast(num_subcarriers, tf.float32)), 0.)\n",
    "\n",
    "        # Scale measurements\n",
    "        h_meas *= tf.complex(tf.sqrt(scaling_factor), 0.)\n",
    "\n",
    "        # Compute losses\n",
    "        h_rt = sionna.utils.flatten_dims(h_rt, 3, 0)\n",
    "        h_meas = sionna.utils.flatten_dims(h_meas, 3, 0)\n",
    "        # Compute the average of a power and delay spread loss\n",
    "        loss_ds = delay_spread_loss(h_rt, h_meas)\n",
    "        loss_pow = power_loss(h_rt, h_meas)\n",
    "\n",
    "        return loss_ds, loss_pow\n",
    "\n",
    "    for step in range(num_iterations):\n",
    "\n",
    "        # Next set of traced paths\n",
    "        next_item = next(training_set_iter, None)\n",
    "\n",
    "        # Retreive the receiver position separately\n",
    "        rx_pos, h_meas, traced_paths = next_item[0], next_item[1], next_item[2:]\n",
    "        # Skip iteration if does not match the batch size\n",
    "        if rx_pos.shape[0] != batch_size:\n",
    "            continue\n",
    "\n",
    "        # Batchify\n",
    "        traced_paths = batchify(traced_paths)\n",
    "\n",
    "        # Train\n",
    "        tr_quantities = training_step(rx_pos, h_meas, traced_paths)\n",
    "        loss_ds_pow, loss_ds, loss_pow, scaling_factor = tr_quantities\n",
    "\n",
    "        # Logging\n",
    "        if (step % 100) == 0:\n",
    "            with train_summary_writer.as_default():\n",
    "                # Log in TB\n",
    "                tf.summary.scalar('loss_ds_pow_training', loss_ds_pow.numpy(), step=step)\n",
    "                tf.summary.scalar('loss_ds_training', loss_ds.numpy(), step=step)\n",
    "                tf.summary.scalar('loss_pow_training', loss_pow.numpy(), step=step)\n",
    "                tf.summary.scalar('scaling_factor', scaling_factor.numpy(), step=step)\n",
    "                # Save model\n",
    "                with open(weights_filename, 'wb') as f:\n",
    "                    pickle.dump(scaling_factor, f)\n",
    "\n",
    "        # Evaluate periodically on the evaluation set\n",
    "        if ((step+1) % 1000) == 0:\n",
    "            eval_loss_ds = 0.0\n",
    "            eval_loss_pow = 0.0\n",
    "            for _ in range(num_validation_iter):\n",
    "                # Next set of traced paths\n",
    "                next_item = next(validation_set_iter, None)\n",
    "\n",
    "                # Retreive the receiver position separately\n",
    "                rx_pos, h_meas, traced_paths = next_item[0], next_item[1], next_item[2:]\n",
    "                # Skip iteration if does not match the batch size\n",
    "                if rx_pos.shape[0] != batch_size:\n",
    "                    continue\n",
    "\n",
    "                # Batchify\n",
    "                traced_paths = batchify(traced_paths)\n",
    "\n",
    "                # Train\n",
    "                eval_quantities = evaluation_step(rx_pos, h_meas, traced_paths, scaling_factor)\n",
    "                loss_ds, loss_pow = eval_quantities\n",
    "                eval_loss_ds += loss_ds\n",
    "                eval_loss_pow += loss_pow\n",
    "            eval_loss_ds /= float(num_validation_iter)\n",
    "            eval_loss_pow /= float(num_validation_iter)\n",
    "            # Log in TB\n",
    "            with train_summary_writer.as_default():\n",
    "                tf.summary.scalar('loss_ds_evaluation', eval_loss_ds, step=step)\n",
    "                tf.summary.scalar('loss_pow_evaluation', eval_loss_pow, step=step)\n",
    "\n",
    "    # Save model\n",
    "    with open(weights_filename, 'wb') as f:\n",
    "        pickle.dump(scaling_factor, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36cf9d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e92ac65f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
